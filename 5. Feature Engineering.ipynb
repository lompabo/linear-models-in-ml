{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Language Recognition\n",
    "\n",
    "**Let's consider the following problem:**\n",
    "\n",
    "* Given a short piece of text (e.g. a sentence)\n",
    "* We want to determine in which language it is written (English/French/Italian)\n",
    "\n",
    "Some examples:\n",
    "\n",
    "> The doctor, who was the family physician, saluted him, but he scarcely took any notice. **--> English**\n",
    "\n",
    "> J'ai couru chez toi, je ne t'ai plus trouvée, tu sais la parole que je t'avais donnée, je la Bens. **--> French**\n",
    "\n",
    "> Conosci tu qualche hossanieh poco scrupoloso che si possa comperare con un bel pugno d'oro? **--> Italian**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Look at the Data\n",
    "\n",
    "**We can find the data for this problem in the `data` folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_test.txt  lr_train.txt  real_estate.csv  weather.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Specifically, we care about the `lr_train.txt` and `lr_test.txt` files\n",
    "* Let's have a look at their content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The doctor, who was the family physician, saluted him, but he scarcely took any notice.@english\n",
      "\n",
      "Cette réserve faite, et faite en toute sévérité, il nous est impossible de ne pas admirer, qu'ils réussissent ou non, les glorieux combattants de l'avenir, les confesseurs de l'utopie.@french\n",
      "\n",
      "\"I have often wondered,\" he said, \"why there was a kind of a Christian air about this one confectioner's shop.@english\n",
      "\n",
      "Ammantellato accuratamente, egli passeggiava innanzi e indietro, colla testa china sul petto, la fronte aggrottata e gli occhi accesi da una cupa fiamma.@italian\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/lr_train.txt') as fp:\n",
    "    lines = fp.readlines()[:4]\n",
    "    print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Extraction\n",
    "\n",
    "**This is raw text data**\n",
    "\n",
    "How can we deal that using a linear model?\n",
    "\n",
    "* We need to find a _numeric encoding_\n",
    "* ...But a one-hot encoding will not longer be enough\n",
    "\n",
    "**The traditional approach in this situation consist in:**\n",
    "\n",
    "* Applying algorithms to the data in order to extract numerical _features_\n",
    "* Use the features as input for the linear model\n",
    "\n",
    "**The task of finding good features is also known as _feature engineering_**\n",
    "\n",
    "It is a critical step in classical Machine Learning\n",
    "\n",
    "* Good features will lead to good results and vice-versa\n",
    "* It can also be a challenging task!\n",
    "\n",
    "The success of Deep Learning is partly due to the possibility to skip this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loading the Data\n",
    "\n",
    "**We will rely on a custom module**\n",
    "\n",
    "...This is called `util.pyu` and is available in the `util` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  __pycache__  util.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the module, we call the `load_lr_data` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The doctor, who was the family physician, saluted him, but he scarcely took any notice. --> english\n",
      "Cette réserve faite, et faite en toute sévérité, il nous est impossible de ne pas admirer, qu'ils réussissent ou non, les glorieux combattants de l'avenir, les confesseurs de l'utopie. --> french\n",
      "\"I have often wondered,\" he said, \"why there was a kind of a Christian air about this one confectioner's shop. --> english\n"
     ]
    }
   ],
   "source": [
    "from util import util\n",
    "\n",
    "text_train, lang_train = util.load_lr_data('data/lr_train.txt')\n",
    "text_test, lang_test = util.load_lr_data('data/lr_test.txt')\n",
    "\n",
    "for text, lang in zip(text_train[:3], lang_train[:3]):\n",
    "    print(f'{text} --> {lang}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Extraction in Practice\n",
    "\n",
    "**We can build a function to process all pieces of text**\n",
    "\n",
    "* For each piece of text, we will run simple algorithms and extract numeric features\n",
    "* These features specifically are _not very good_\n",
    "* Later, we will work to improve them!\n",
    "\n",
    "We call the function for both our training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unidecode\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Here is a list of all separators found in the data\n",
    "separators = ['(', '«', ',', '»', u\"'\", '+', '/', '°', ';', '?', '[', '_', ' ', '\\n', '\"', '&', '*', '.', ':', '\\t', '!', ')', '-', 'º', ']', ]\n",
    "separators_as_string = ''.join(separators)\n",
    "\n",
    "def extract_features(text):\n",
    "    # Prepare a data structure to store the results\n",
    "    res = []\n",
    "    names = []\n",
    "    # Loop over all pieces of text\n",
    "    for cnt, lne in enumerate(text):\n",
    "        # PREPARE A ROW FOR THE DATASET\n",
    "        row = []\n",
    "        # Obtain the list of words (this is useful to extract many features)\n",
    "        words = [w.strip(separators_as_string) for w in lne.split()]\n",
    "        words = [w for w in words if len(w) > 0]\n",
    "\n",
    "        # EXTRACT FEATURES\n",
    "        # F0: Number of words\n",
    "        nwords = len(words)\n",
    "        row.append(nwords)\n",
    "        if cnt == 0: names.append('nwords')\n",
    "        # F1: Number of characters per word\n",
    "        row.append(len(lne) / nwords)\n",
    "        if cnt == 0: names.append('ncpw')\n",
    "        # F2: Get number of vowels\n",
    "        # NOTE: \"lower()\" makes the string lower case\n",
    "        nvowels = len([c for c in lne if c.lower() in ('a', 'e', 'i', 'o', 'u', 'y')])\n",
    "        row.append(nvowels)\n",
    "        if cnt == 0: names.append('nvowels')\n",
    "        # F3: Get the number of words starting with \"p\"\n",
    "        nstartp = len([w for w in words if w[0] == 'p'])\n",
    "        row.append(nstartp)\n",
    "        if cnt == 0: names.append('nstartp')\n",
    "        # F4: Get number of occurrences of either \"cookie\" or \"wookie\"\n",
    "        ncwookie = len([w for w in words if w in ['cookie', 'wookie']])\n",
    "        row.append(ncwookie)\n",
    "        if cnt == 0: names.append('ncwookie')\n",
    "        \n",
    "        # APPEND THE ROW\n",
    "        res.append(row)\n",
    "    # Return the result\n",
    "    return pd.DataFrame(columns=names, data= res)\n",
    "\n",
    "\n",
    "x_train = extract_features(text_train)\n",
    "x_test = extract_features(text_test)\n",
    "\n",
    "# Rescale\n",
    "scaler = StandardScaler()\n",
    "x_train = pd.DataFrame(columns=x_train.columns, data=scaler.fit_transform(x_train))\n",
    "x_test = pd.DataFrame(columns=x_test.columns, data=scaler.transform(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Extraction in Practice\n",
    "\n",
    "**We can now have a look at the result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The doctor, who was the family physician, saluted him, but he scarcely took any notice.\n",
      "Cette réserve faite, et faite en toute sévérité, il nous est impossible de ne pas admirer, qu'ils réussissent ou non, les glorieux combattants de l'avenir, les confesseurs de l'utopie.\n",
      "\"I have often wondered,\" he said, \"why there was a kind of a Christian air about this one confectioner's shop.\n",
      "Ammantellato accuratamente, egli passeggiava innanzi e indietro, colla testa china sul petto, la fronte aggrottata e gli occhi accesi da una cupa fiamma.\n",
      "--Tu mi chiedevi il perchè ti seppellii in quest'inferno, diss'egli, calcando su ogni parola.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwords</th>\n",
       "      <th>ncpw</th>\n",
       "      <th>nvowels</th>\n",
       "      <th>nstartp</th>\n",
       "      <th>ncwookie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.781330</td>\n",
       "      <td>-0.207766</td>\n",
       "      <td>-0.801322</td>\n",
       "      <td>-0.188279</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.605359</td>\n",
       "      <td>0.633761</td>\n",
       "      <td>0.649457</td>\n",
       "      <td>-0.188279</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.286084</td>\n",
       "      <td>-0.671139</td>\n",
       "      <td>-0.473727</td>\n",
       "      <td>-0.921664</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011063</td>\n",
       "      <td>1.108481</td>\n",
       "      <td>0.509059</td>\n",
       "      <td>0.545107</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.880380</td>\n",
       "      <td>1.094091</td>\n",
       "      <td>-0.707723</td>\n",
       "      <td>0.545107</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     nwords      ncpw   nvowels   nstartp  ncwookie\n",
       "0 -0.781330 -0.207766 -0.801322 -0.188279       0.0\n",
       "1  0.605359  0.633761  0.649457 -0.188279       0.0\n",
       "2 -0.286084 -0.671139 -0.473727 -0.921664       0.0\n",
       "3  0.011063  1.108481  0.509059  0.545107       0.0\n",
       "4 -0.880380  1.094091 -0.707723  0.545107       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for text in text_train[:5]: print(text)\n",
    "display(x_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now have a usable dataset!\n",
    "* Just don't count on the initial feature to be any good ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multinomial/Multiclass Classification\n",
    "\n",
    "**We now need to take care of the targets**\n",
    "\n",
    "...Which in a classification setting are often referred to as _labels_\n",
    "\n",
    "* Unlike in the previous example, we have three classes\n",
    "* ...Wich makes this a _multinomial (or multiclass) classification_ task\n",
    "\n",
    "**We can deal with that by:**\n",
    "\n",
    "* Introducing _one separate Logistic Regression model per class_\n",
    "  - Hence, in our case we will have three models and three outputs\n",
    "  - Normalize the outputs over their sum, so they can be seen as a probability distribution\n",
    "* Using the categorical (rather than binomial) crossentropy as loss function\n",
    "\n",
    "**Fortunately, scikit-learn takes care of all these steps for us**\n",
    "\n",
    "We just need to add the class as an integer (or categorical) attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_train = pd.Series(lang_train).astype('category')\n",
    "lang_test = pd.Series(lang_test).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multinomial Logistic Regression in Practice\n",
    "\n",
    "**We will train a Logistic Regression model with L1 regularization**\n",
    "\n",
    "...And we will use grid search + cross-validation to tune the reguralizer weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "m = LogisticRegression(penalty='l1', C=1.0, solver='saga', tol=0.01)\n",
    "param_grid = {'C': np.logspace(-2, 2, 10)}\n",
    "gscv = GridSearchCV(estimator=m, param_grid=param_grid, cv=5)\n",
    "gscv.fit(x_train, lang_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `C` parameter is the inverse of the reguralizer weight\n",
    "* We use the [SAGA](https://arxiv.org/abs/1407.0202) since the default one does not support L1 reguralization\n",
    "* We reduce the tolerance to 0.01 to reach convergence within the default iteration limit\n",
    "\n",
    "Then we can inspect the value of the best parameter and otbain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.5994842503189409}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "**We can now obtain the predictions and evaluate the model (on the test set)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff3aa7b73eb489688d97868e69dc86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_ts = gscv.predict(x_test)\n",
    "print(f'Accuracy on the test set: {accuracy_score(lang_test, y_pred_ts):.3}')\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(gscv, x_test, lang_test, display_labels=lang_test.cat.categories, cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "**Since we used L1 regularization & rescaling, we can interpret the coeffients**\n",
    "\n",
    "We have one coefficient per input and per output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.69187217,  0.28186408, -4.84931909, -0.78323603,  0.        ],\n",
       "       [ 0.3647371 ,  0.24070275, -0.33609433,  0.39100536,  0.        ],\n",
       "       [-5.32233124, -0.66603289,  5.47030616,  0.10644806,  0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79253ed9f41e40f3a016453479af5eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "n = len(x_train.columns)\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.pcolor(gscv.best_estimator_.coef_, cmap='RdBu')\n",
    "plt.xticks(0.5 + np.arange(len(x_train.columns)), x_train.columns);\n",
    "plt.yticks(0.5 + np.arange(len(lang_train.cat.categories)), lang_train.cat.categories);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's Put Everything Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's Put Everything Together\n",
    "\n",
    "**Finally, we can put everything together**\n",
    "\n",
    "We can improve our model by:\n",
    "\n",
    "* Extracting features\n",
    "* Training a model (and calibrating $C$)\n",
    "* Checking _which kind of mistakes we are making_\n",
    "* Checking _which features are actually useful_\n",
    "\n",
    "...And the using the collected information to devise new features!\n",
    "\n",
    "**You can find the process for a whole iteration in a single cell, for convenience:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772f3ab18ef04bdf9017c8d6f2c2993c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d912594e5b2d49e4a561b40c7e44b685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_features(text):\n",
    "    # Prepare a data structure to store the results\n",
    "    res = []\n",
    "    names = []\n",
    "    # Loop over all pieces of text\n",
    "    for cnt, lne in enumerate(text):\n",
    "        # PREPARE A ROW FOR THE DATASET\n",
    "        row = []\n",
    "        # Obtain the list of words (this is useful to extract many features)\n",
    "        words = [w.strip(separators_as_string) for w in lne.split()]\n",
    "        words = [w for w in words if len(w) > 0]\n",
    "\n",
    "        # EXTRACT FEATURES\n",
    "        # F0: Number of words\n",
    "        nwords = len(words)\n",
    "        row.append(nwords)\n",
    "        if cnt == 0: names.append('nwords')\n",
    "        # F1: Number of characters per word\n",
    "        row.append(len(lne) / nwords)\n",
    "        if cnt == 0: names.append('ncpw')\n",
    "        # F2: Get number of vowels\n",
    "        # NOTE: \"lower()\" makes the string lower case\n",
    "        nvowels = len([c for c in lne if c.lower() in ('a', 'e', 'i', 'o', 'u', 'y')])\n",
    "        row.append(nvowels)\n",
    "        if cnt == 0: names.append('nvowels')\n",
    "        # F3: Get the number of words starting with \"p\"\n",
    "        nstartp = len([w for w in words if w[0] == 'p'])\n",
    "        row.append(nstartp)\n",
    "        if cnt == 0: names.append('nstartp')\n",
    "        # F4: Get number of occurrences of either \"cookie\" or \"wookie\"\n",
    "        ncwookie = len([w for w in words if w in ['cookie', 'wookie']])\n",
    "        row.append(ncwookie)\n",
    "        if cnt == 0: names.append('ncwookie')\n",
    "        \n",
    "        # APPEND THE ROW\n",
    "        res.append(row)\n",
    "    # Return the result\n",
    "    return pd.DataFrame(columns=names, data= res)\n",
    "\n",
    "\n",
    "x_train = extract_features(text_train)\n",
    "x_test = extract_features(text_test)\n",
    "\n",
    "# Rescale\n",
    "scaler = StandardScaler()\n",
    "x_train = pd.DataFrame(columns=x_train.columns, data=scaler.fit_transform(x_train))\n",
    "x_test = pd.DataFrame(columns=x_test.columns, data=scaler.transform(x_test))\n",
    "\n",
    "# Train a model and calibrate the reguralizer weight\n",
    "m = LogisticRegression(penalty='l1', C=1.0, solver='saga', tol=0.01)\n",
    "param_grid = {'C': np.logspace(-2, 2, 10)}\n",
    "gscv = GridSearchCV(estimator=m, param_grid=param_grid, cv=5)\n",
    "gscv.fit(x_train, lang_train);\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "y_pred_ts = gscv.predict(x_test)\n",
    "print(f'Accuracy on the test set: {accuracy_score(lang_test, y_pred_ts):.3}')\n",
    "\n",
    "plot_confusion_matrix(gscv, x_test, lang_test, display_labels=lang_test.cat.categories, cmap='Blues')\n",
    "\n",
    "n = len(x_train.columns)\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.pcolor(gscv.best_estimator_.coef_, cmap='RdBu')\n",
    "plt.xticks(0.5 + np.arange(len(x_train.columns)), x_train.columns);\n",
    "plt.yticks(0.5 + np.arange(len(lang_train.cat.categories)), lang_train.cat.categories);"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "center": false,
   "transition": "fade"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
