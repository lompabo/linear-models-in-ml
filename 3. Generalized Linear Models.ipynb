{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload all imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Allow to pan and zoom graphs & charts\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Impact of Individual Attributes\n",
    "\n",
    "**We may be interested in understanding the impact of individual attributes**\n",
    "\n",
    "<center><img src=\"assets/taiwan-tea-house.jpg\" style=\"height: 400px;\"></center>\n",
    "\n",
    "* E.g. is the price per area more impacted age, distance from MRT stations, or what? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Straightforward Approach\n",
    "\n",
    "**Let's start by repeating our basic linear regression approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.704 (training), 0.618 (test)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "data = pd.read_csv('data/real_estate.csv', sep=';')\n",
    "cols = data.columns\n",
    "X = data[cols[:-1]]\n",
    "y = np.log(data[cols[-1]])\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.34, random_state=0)\n",
    "\n",
    "m = LinearRegression()\n",
    "m.fit(X_tr, y_tr);\n",
    "\n",
    "y_pred_tr, y_pred_ts = m.predict(X_tr), m.predict(X_ts)\n",
    "print(f'R2: {r2_score(y_tr, y_pred_tr):.3} (training), {r2_score(y_ts, y_pred_ts):.3} (test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We load the dataset, separate input & output, split the training and test set\n",
    "* The we fit the model and evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Straightforward Approach\n",
    "\n",
    "**We can think of assessing the attribute importance by looking at the coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358b5a30ef5044f483ab9ab9278cdcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "idx = np.argsort(-np.abs(m.coef_)) # Sort indexes by decreasing absolute coefficient\n",
    "scores = m.coef_[idx] # Coefficients, in the order defined by idx\n",
    "names = X.columns[idx] # Corresponding names\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.bar(0.5 + np.arange(len(idx)), scores, tick_label=names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Intuition: the higher the (absolute) weight, the more important the attribute\n",
    "* Positive coefficient = price increase, negative coefficient = price decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Straightforward Approach\n",
    "\n",
    "**There is a (huge) pitfall with this approach**\n",
    "\n",
    "* Each coefficient value does not only depend on the importnat of an attribute\n",
    "* ...But also on its scale!\n",
    "\n",
    "An attribute with _larger values_ may end up with a _smaller coefficient_\n",
    "\n",
    "**We can preprocess the data so that all attributes have _similar scales_**\n",
    "\n",
    "A couple of common approaches:\n",
    "\n",
    "* Normalization (all values in the 0-1 range)\n",
    "$$\n",
    "x^\\prime = \\frac{x - \\min(x)}{\\max{x} - \\min{x}}\n",
    "$$\n",
    "* Standardization (the mean becomes 0, the st. dev. becomes 1):\n",
    "$$\n",
    "x^\\prime = \\frac{x - \\text{mean}(x)}{\\text{std}{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalization/Standardization\n",
    "\n",
    "**Scikit learn provides simple tools to take care of this step**\n",
    "\n",
    "The are called `MinMaxScaler` and `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # for standardization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_tr_s = scaler.fit_transform(X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the training set:\n",
    "\n",
    "* We calibrate the scaler (i.e. compute the mean and st. dev.)\n",
    "* The we apply the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts_s = scaler.transform(X_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set we simply apply the transformation\n",
    "\n",
    "* We do not want indirect information about the test data to \"leak\" into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Correct Approach\n",
    "\n",
    "**We can then repeat the process using the scaled data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the test set: 0.618\n"
     ]
    }
   ],
   "source": [
    "m = LinearRegression()\n",
    "m.fit(X_tr_s, y_tr);\n",
    "\n",
    "y_pred_ts = m.predict(X_ts_s)\n",
    "print(f'R2 on the test set: {r2_score(y_ts, y_pred_ts):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The quality of the predictions is the same as before\n",
    "* ...Since we have applied only a linear transformation to the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Correct Approach\n",
    "\n",
    "**Now we can build and inspect the same plot as before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4082f802384ec49db4fd779fd078e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.argsort(-np.abs(m.coef_)) # Sort indexes by decreasing absolute coefficient\n",
    "scores = m.coef_[idx] # Coefficients, in the order defined by idx\n",
    "names = X.columns[idx] # Corresponding names\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.bar(0.5 + np.arange(len(idx)), scores, tick_label=names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The order of the attributes has changed!\n",
    "* Positive/negative values = positive/negative effect if above average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attribute Selection\n",
    "\n",
    "**Let's look again at the last plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041da367690847058748a13d60416a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "plt.bar(0.5 + np.arange(len(idx)), scores, tick_label=names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The \"longitude\" attribute has very low importance\n",
    "* Given that, is it worth keeping the attribute at all?\n",
    "* Collecting attributes has a cost, so keeping useless ones around is not a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attribute Selection\n",
    "\n",
    "**The problem of choosing which attributes to keep is known as _attribute selection_**\n",
    "\n",
    "There are multiple solution approaches\n",
    "\n",
    "* An exact (but costly) one: consider all possible sets and repeat training\n",
    "* A cheap (but heuristic) one: remove attributes having coefficients with small absolute value\n",
    "\n",
    "**In the case of linear regression, we have an interesting option**\n",
    "\n",
    "We can _act on the structure of the loss (error) function_ used during training:\n",
    "$$\n",
    "L(w) = \\mathit{MSE}(w) + \\alpha \\|w\\|_1\n",
    "$$\n",
    "\n",
    "* Where $\\|w\\|_1 = \\sum_{j=1}^n |w_j|$\n",
    "* The additional term $\\|w\\|$ is introduced to make mode model \"well behaved\"\n",
    "* ...And for this reason it is called a _reguralizer_\n",
    "* In particular, since we are using the L1 norm, we call it an _L1 regularizer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LASSO\n",
    "\n",
    "**At training time, we will need to solve**\n",
    "\n",
    "$$\n",
    "\\text{argmin}_w \\mathit{MSE}(w) + \\alpha \\|w\\|_1\n",
    "$$\n",
    "\n",
    "\n",
    "Intuitively:\n",
    "\n",
    "* Changing a $w_j$ makes sense only if we can reduce the error\n",
    "* The reduction will occur with a certain slope (or \"rate\")\n",
    "* If this slow cannot become (sufficienly) higher than $\\alpha$ \n",
    "* ...It will be better to keep $w_j$ fixed at 0\n",
    "\n",
    "**As a result, we naturally end-up doing attribute selection**\n",
    "\n",
    "The resulting approach is known as _Least Absolute Shrinkage and Selection Operator_\n",
    "\n",
    "* ...Or LASSO for short :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using LASSO\n",
    "\n",
    "**Scikit-learn provides a convenient LASSO implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "m2 = Lasso(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When building the model, we need to pick a value for $\\alpha$\n",
    "* Higher $\\alpha$ values promote using fewer attributes\n",
    "* ...Bu they also reduce the prediction quality\n",
    "\n",
    "**For now, we will fix $\\alpha$ via trial-and-error approach:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the training set: 0.0\n"
     ]
    }
   ],
   "source": [
    "m2.fit(X_tr_s, y_tr);\n",
    "y_pred_tr = m2.predict(X_tr_s)\n",
    "print(f'R2 on the training set: {r2_score(y_tr, y_pred_tr):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is too low, and therefore $\\alpha = 1.0$ is too large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Choosing $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the training set: 0.572\n"
     ]
    }
   ],
   "source": [
    "m2 = Lasso(alpha=0.1)\n",
    "m2.fit(X_tr_s, y_tr);\n",
    "y_pred_tr = m2.predict(X_tr_s)\n",
    "print(f'R2 on the training set: {r2_score(y_tr, y_pred_tr):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The prediction quality is still much lower than the original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the training set: 0.702\n"
     ]
    }
   ],
   "source": [
    "m2 = Lasso(alpha=0.01)\n",
    "m2.fit(X_tr_s, y_tr);\n",
    "y_pred_tr = m2.predict(X_tr_s)\n",
    "print(f'R2 on the training set: {r2_score(y_tr, y_pred_tr):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we are back to the original value (more or less)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Impact of Each Attribute\n",
    "\n",
    "**Once we are satisfied with the $\\alpha$ values, we can inspect the coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3de3b98146444383cf0b3be9dd2794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.argsort(-np.abs(m2.coef_))\n",
    "scores = m2.coef_[idx]\n",
    "names = X.columns[idx]\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.bar(0.5 + np.arange(len(idx)), scores, tick_label=names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The \"longitude\" attribute has been dropped\n",
    "* When the dataset contains a large number of attributes, the LASSO method can be extremely useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "**We have already observed how our model has a modest degree of overfitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the training set (linear regression): 0.702\n",
      "R2 on the test set (linear regression): 0.618\n"
     ]
    }
   ],
   "source": [
    "print(f'R2 on the training set (linear regression): {r2_score(y_tr, y_pred_tr):.3}')\n",
    "print(f'R2 on the test set (linear regression): {r2_score(y_ts, y_pred_ts):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting arises when:**\n",
    "\n",
    "* The model learns patterns in the training set\n",
    "* ...That do not repeat on unseen data (e.g. since they due to random sampling)\n",
    "\n",
    "**This becomes more critical with many attributes and scarce data**\n",
    "\n",
    "* A linear regression model with $n$ coefficients\n",
    "* ...Can interpolate perfectly any collections of $n$ examples\n",
    "\n",
    "...But typically, that means we learning noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First, Let's Make It Worse\n",
    "\n",
    "**First, let's amplify the problem, so as to make any change more significant**\n",
    "\n",
    "In particular, we will reduce the amount of available training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 41\n",
      "R2: 0.771 (training), 0.555 (test)\n"
     ]
    }
   ],
   "source": [
    "X_tr2, X_ts2, y_tr2, y_ts2 = train_test_split(X, y, test_size=0.90, random_state=0)\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "X_tr_s2 = scaler2.fit_transform(X_tr2)\n",
    "X_ts_s2 = scaler2.transform(X_ts2)\n",
    "print(f'Training set size: {len(X_tr_s2)}')\n",
    "\n",
    "m3 = LinearRegression()\n",
    "m3.fit(X_tr2, y_tr2)\n",
    "\n",
    "y_pred_tr3, y_pred_ts3 = m3.predict(X_tr2), m3.predict(X_ts2)\n",
    "print(f'R2: {r2_score(y_tr2, y_pred_tr3):.3} (training), {r2_score(y_ts2, y_pred_ts3):.3} (test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Having access to little data is not uncommon\n",
    "* Raw measurements are easy to obtain, but targets may be expensive to collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ridge Regression\n",
    "\n",
    "**We will show how to reduce overfitting by using _ridge regression_**\n",
    "\n",
    "...Which is obtained from linear regression by adding a quadratic reguralizer:\n",
    "$$\n",
    "\\text{argmin}_w \\mathit{MSE}(w) + \\alpha \\|w\\|_2^2\n",
    "$$\n",
    "where $\\|w\\|_2^2 = \\sum_{j=1}^2 w_j^2$\n",
    "\n",
    "* Since the $\\|w\\|_2$ is an L2 norm, this is called an _L2 regularizer_\n",
    "* It looks similar to the L1 regularizer in the LASSO approach\n",
    "* ...But it has a very different effect\n",
    "\n",
    "**The regularizer generally _prevents weights from having a large absolute value_**\n",
    "\n",
    "* Smaller (absolute) weights result in smoother behavior\n",
    "* ...Which typically leads to better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using Ridge Regression\n",
    "\n",
    "**Scikit-learn provides an implementation for Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "m4 = Ridge(alpha=1.0)\n",
    "m4.fit(X_tr_s2, y_tr2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works similarly to the `Lasso` class\n",
    "\n",
    "* When we build the model, we need to specify a value for $\\alpha$\n",
    "* ...Then we train the model as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the training set: 0.771\n",
      "R2 on the test set: 0.567\n"
     ]
    }
   ],
   "source": [
    "y_pred_tr4 = m4.predict(X_tr_s2)\n",
    "y_pred_ts4 = m4.predict(X_ts_s2)\n",
    "print(f'R2 on the training set: {r2_score(y_tr2, y_pred_tr4):.3}')\n",
    "print(f'R2 on the test set: {r2_score(y_ts2, y_pred_ts4):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The default value for $\\alpha$ is 1.0\n",
    "* ...And it is already helping a bit the model on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-Validation and Hyper-Parameter Tuning\n",
    "\n",
    "**This time, we will follow a systematic approach to pick a value for $\\alpha$**\n",
    "\n",
    "* Our goal is to improve the model results on unseen examples\n",
    "* So we can choose $\\alpha$ so as to minimize the error on example _not in the training set_\n",
    "\n",
    "**It would be tempting to optimize $\\alpha$ for the test set...**\n",
    "\n",
    "...But it would also be _very wrong_\n",
    "\n",
    "* The test set is our ultimate approach to assess generalization\n",
    "* For this, it's crucial that its content _stay unseen_ whenever we calibrate parameters\n",
    "\n",
    "**We can however split _yet another set_ on this purpose**\n",
    "\n",
    "* This would not be used to train the model\n",
    "* ...But only for tuning the parameter of the training approach (called _hyper-parameters_)\n",
    "\n",
    "This is called a _validation set_\n",
    "\n",
    "* But there is a drawback: we have _even less data available for training_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-Validation and Hyper-Parameter Tuning\n",
    "\n",
    "**As an alternative, we can use _$k$-fold cross-validation_:**\n",
    "\n",
    "* We partition the training data in $k$ sets, called _folds_\n",
    "* For $i = 1..k$:\n",
    "  - We separate the $i$-th fold\n",
    "  - We train a model on the remaining data\n",
    "  - We evaluate the prediction quality on the separated fold\n",
    "  - Then we put the fold back\n",
    "  \n",
    "During each iteration, the examples in the $i$-th folder are effectively unseen\n",
    "\n",
    "**At the end of the process we have:**\n",
    "\n",
    "* $k$ models (that typically we _don't_ use)\n",
    "* $k$ prediction quality evaluations (of which we can take the mean)\n",
    "\n",
    "We can use cross validation to _assess how good a model is with unseen data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applying Cross-Validation and Hyper-Parameter Tuning\n",
    "\n",
    "**Scikit learn allows us to automate this process with a few lines of code**\n",
    "\n",
    "* First we specify which parameters we want to tune\n",
    "* ...And which range we want to investigate\n",
    "\n",
    "We do so by building a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ranges = {'alpha': np.logspace(0, 2, 100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dictionary keys are the parameters to be tuned\n",
    "* The entries are the values to be tested\n",
    "\n",
    "Then we build a `GridSearchCV` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gscv = GridSearchCV(estimator=Ridge(), param_grid=param_ranges, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `cv` parameter specifies the number of folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applying Cross-Validation and Hyper-Parameter Tuning\n",
    "\n",
    "**The we can \"train\" the `GridSearchCV` object**\n",
    "\n",
    "* This will try all combinations of the values in parameter ranges\n",
    "* ...And pick the parameters leading to the best cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.fit(X_tr_s2, y_tr2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters can be accessed via the `best_params_` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 12.91549665014884}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applying Cross-Validation and Hyper-Parameter Tuning\n",
    "\n",
    "**By default, the `GridSearchCV` object:**\n",
    "\n",
    "* Trains an estimator with the best parameters _on all data_\n",
    "* This is the model that we are going to use\n",
    "\n",
    "The `predict` method on the `GridSearchCV` object calls the same method on the best estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the training set: 0.75\n",
      "R2 on the test set: 0.625\n"
     ]
    }
   ],
   "source": [
    "y_pred_tr4 = gscv.predict(X_tr_s2)\n",
    "y_pred_ts4 = gscv.predict(X_ts_s2)\n",
    "print(f'R2 on the training set: {r2_score(y_tr2, y_pred_tr4):.3}')\n",
    "print(f'R2 on the test set: {r2_score(y_ts2, y_pred_ts4):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is _beating our original approach_ on the test set\n",
    "* ...Even if we using _only 41 data points_!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "center": false,
   "transition": "fade"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
